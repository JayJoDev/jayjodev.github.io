---
title: Stochastic Gradient Descent (SGD)
author: Jay Jo
date: 2023-11-29 00:00:00 +09:00
categories: [Artificial Intelligence]
tags: [AI]
---

# Stochastic Gradient Descent (SGD)

## 개요

확률적 경사 하강법(Stochastic Gradient Descent, SGD)은 최적화 문제, 특히 대규모 머신 러닝 모델을 학습시키는 데 자주 사용되는 기법입니다. 이 방법은 비용 함수의 그래디언트를 계산하여 모델의 매개변수를 점진적으로 조정하는 기법입니다.

## 작동 원리

SGD는 다음과 같은 단계로 작동합니다:

1. **그래디언트 계산**: 모델의 매개변수에 대한 비용 함수의 기울기(그래디언트)를 계산합니다. 이 기울기는 오차를 최소화하는 방향을 가리킵니다.

2. **매개변수 업데이트**: 계산된 그래디언트를 사용하여 매개변수를 업데이트합니다. 이 때, 학습률(`learning rate`)이 업데이트의 크기를 결정합니다.

    ```python
    # 매개변수 업데이트 예시 코드
    parameter = parameter - learning_rate * gradient
    ```

3. **반복 수행**: 데이터 세트에 대해 이 과정을 여러 번 반복하며 매개변수를 조정합니다.

## 수식

SGD의 업데이트 규칙은 다음과 같은 수식으로 나타낼 수 있습니다:


$$
\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)} )
$$


여기서:

$$
\begin{align*}
& 1. \ \theta \text{는 매개변수,} \\
& 2. \ \eta \text{는 학습률,} \\
& 3. \ \nabla_\theta J( \theta; x^{(i)}; y^{(i)} ) \text{는 비용 함수 } J \text{에 대한 } \theta \text{의 그래디언트 입니다.}
\end{align*}
$$

## 특징

- **효율성**: 전체 데이터 세트를 한 번에 사용하지 않고, 무작위로 선택된 샘플을 사용하여 그래디언트를 계산합니다. 이로 인해 대규모 데이터 세트에 효율적입니다.

- **유연성**: 다양한 종류의 문제와 모델에 적용될 수 있습니다.

- **일반화**: 매 반복마다 다른 샘플을 사용함으로써, 모델의 일반화 능력을 향상시킬 수 있습니다.

## 주의점

- **학습률 선택**: 학습률이 너무 높으면 발산할 수 있고, 너무 낮으면 학습이 매우 느려질 수 있습니다.

- **최적화 문제**: SGD는 지역 최적해(local minima)나 안장점(saddle point)에 빠질 수 있습니다.

## 결론

SGD는 머신 러닝에서 널리 사용되는 강력한 최적화 방법이며, 특히 대규모 데이터 세트에 적합한 방법입니다.
