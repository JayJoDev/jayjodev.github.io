---
title: 기계학습 기초[클러스터링]
author: Jay Jo
date: 2023-11-26 00:00:00 +09:00
categories: [Artificial Intelligence]
tags: [ai]
image: /assets/img/posts/clustering.png
---

# K-평균 클러스터링(K-means clustering)

**비지도 학습(Unsupervised Learning)** 의 한 예입니다. 비지도 학습은 정답(레이블) 없이 입력 데이터만을 사용하여 학습을 진행하는 방법입니다. 이러한 방식은 데이터의 내재된 패턴, 구조, 상관관계 등을 파악하는 데 주로 사용됩니다.

### 지도 학습 (Supervised Learning) vs 비지도 학습 (Unsupervised Learning)

| 구분 | 지도 학습 (Supervised Learning) | 비지도 학습 (Unsupervised Learning) |
|------|---------------------------------|--------------------------------------|
| 학습 데이터 | 입력(input)과 정답(label) | 입력만 사용 |
| 정답의 유무 | 정답은 사람이 만든 정보 | 정답 없음 |
| 학습 목적 | 입력과 정답에 대한 관계 학습 | 입력의 패턴 및 상관관계 학습 |
| 예시 | 기존 정보(데이터셋)를 토대로 새로운 입력에 대한 정답 추측 | 클러스터링, 차원 축소 등 |


## K-평균 클러스터링

K-평균 클러스터링 알고리즘은 데이터 포인트들을 K개의 클러스터로 그룹화하는 과정으로, 다음과 같은 단계로 이루어집니다:

1. 초기화:
클러스터의 개수 K를 결정합니다.
데이터셋에서 무작위로 K개의 포인트를 선택하여 각 클러스터의 초기 중심(centroid)으로 설정합니다.

2. 할당 단계:

데이터셋의 각 포인트에 대해, 모든 중심점에 대한 거리를 계산합니다.
각 데이터 포인트를 가장 가까운 중심점을 갖는 클러스터에 할당합니다.

3. 업데이트 단계:
할당된 데이터 포인트들을 기반으로 각 클러스터의 새로운 중심을 계산합니다. 일반적으로 클러스터 내의 모든 포인트의 평균 위치를 새로운 중심으로 합니다.

4. 수렴 확인:
중심점들이 더 이상 변하지 않거나, 정해진 반복 횟수에 도달하거나, 클러스터 할당이 더 이상 변하지 않을 때까지 2단계와 3단계를 반복합니다.
5. 결과 출력:
최종 클러스터 할당과 중심점을 결과로 출력합니다.

```python
import numpy as np

def k_means(X, K, max_iters=100):
    # 데이터 포인트의 수와 특성의 수
    n, features = X.shape
    
    # 중심점 초기화
    centroids = X[np.random.choice(n, K, replace=False)]

    # 클러스터 할당을 위한 배열 초기화
    clusters = np.zeros(n)

    for _ in range(max_iters):
        # 각 데이터 포인트에 대해 가장 가까운 중심점 찾기
        for i, point in enumerate(X):
            distances = np.sqrt(np.sum((point - centroids) ** 2, axis=1))
            clusters[i] = np.argmin(distances)

        # 새로운 중심점 계산
        new_centroids = np.array([X[clusters == k].mean(axis=0) for k in range(K)])

        # 중심점이 더이상 변하지 않으면 종료
        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return clusters, centroids

# 예제 데이터셋 사용
from sklearn.datasets import make_blobs
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# K-평균 클러스터링 실행
clusters, centroids = k_means(X, K=4)

# 클러스터링 결과 시각화
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=clusters, s=50, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='x') # 중심점을 표시
plt.title("K-Means Clustering")
plt.show()
```

## 단점
1. 클러스터의 수 결정 필요: K-평균 알고리즘을 시작하기 전에 클러스터의 수(K)를 사전에 결정해야 합니다. 이는 데이터의 구조가 미리 알려져 있지 않은 경우 문제가 될 수 있습니다.

2. 구형 클러스터 가정: 알고리즘은 각 클러스터를 구형으로 가정합니다. 이는 클러스터가 비구형이거나, 서로 다른 크기와 밀도를 가질 때 비효율적일 수 있습니다.

3. 초기 중심점에 대한 민감성: 초기 중심점의 선택이 결과에 큰 영향을 미칩니다. 잘못된 초기 중심점은 수렴 속도를 늦추거나 지역 최적해(local optimum)에 도달할 수 있습니다.

4. 이상치에 대한 민감함: 이상치는 클러스터의 중심점을 크게 왜곡시킬 수 있어, 결과에 부정적인 영향을 미칠 수 있습니다.

5. 대규모 데이터셋 처리의 어려움: 대규모 데이터셋에서는 K-평균 알고리즘의 계산 비용이 상당히 높을 수 있으며, 특히 고차원 데이터에서 이 문제는 더욱 심해질 수 있습니다.

6. 결과의 일관성 부족: 무작위 초기화로 인해, 알고리즘을 여러 번 실행할 때마다 다른 결과가 나올 수 있습니다.

7. 클러스터 크기와 밀도의 차이: K-평균은 모든 클러스터가 대략적으로 비슷한 크기를 가진다고 가정합니다. 크기나 밀도가 매우 다른 클러스터를 가진 데이터셋에서는 잘 작동하지 않을 수 있습니다.

8. 국소 최적화 문제: 초기 중심점의 선택과 알고리즘의 경사 하강 방식으로 인해, K-평균은 국소 최적화에 빠질 수 있습니다. 즉, 전역 최적해보다 덜 최적인 해에 수렴할 수 있습니다.


## K-평균 클러스터링 평가 방법

K-평균 클러스터링의 성능을 평가하는 데 사용되는 주요 지표들의 설명과 수식입니다.

### 1. 엘보우 방법 (Elbow Method)

클러스터의 수(K)를 결정하는 데 사용됩니다.

- 수식: 
  ```
  WCSS = ∑(i=1 to k) ∑(x ∈ Ci) ||x - μi||²
  ```
  여기서 `k`는 클러스터의 수, `Ci`는 i번째 클러스터, `x`는 클러스터 내의 데이터 포인트, `μi`는 i번째 클러스터의 중심점입니다.

### 2. 실루엣 점수 (Silhouette Score)

각 데이터 포인트의 클러스터 내 코히전(cohesion)과 클러스터 간 분리(separation)를 측정합니다.

- 수식:
  ```
  s(i) = (b(i) - a(i)) / max{a(i), b(i)}
  ```
  여기서 `a(i)`는 동일한 클러스터 내의 다른 데이터 포인트들과의 평균 거리, `b(i)`는 가장 가까운 다른 클러스터의 데이터 포인트들과의 평균 거리입니다.

### 3. 던 인덱스 (Dunn Index)

클러스터 내 최대 거리와 클러스터 간 최소 거리의 비율로 계산됩니다.

- 수식:
  ```
  D = min(1 ≤ i ≤ k) ( min(1 ≤ j ≤ k, j ≠ i) ( δ(Ci, Cj) / max(1 ≤ l ≤ k) ( Δ(Cl) ) ) )
  ```
  여기서 `δ(Ci, Cj)`는 클러스터 `Ci`와 `Cj` 간의 거리, `Δ(Cl)`는 클러스터 `Cl` 내의 최대 거리입니다.

### 4. 다비스-볼딘 인덱스 (Davies-Bouldin Index)

클러스터의 평균 거리와 가장 가까운 클러스터의 평균 거리의 비율을 평균내어 계산합니다.

- 수식:
  ```
  DB = (1/k) ∑(i=1 to k) max(R(i, j)) for all j ≠ i
  ```
  여기서 `R(i, j)`는 클러스터 `Ci`와 `Cj`에 대한 다비스-볼딘의 비율입니다.

### 5. 클러스터 내 분산

클러스터 내의 분산을 최소화하는 것이 K-평균의 목적 중 하나입니다.

- 수식:
  ```
  내부 분산 = ∑(i=1 to k) ∑(x ∈ Ci) ||x - μi||²
  ```
  여기서 `Ci`는 i번째 클러스터, `x`는 클러스터 내의 데이터 포인트, `μi`는 i번째 클러스터의 중심점입니다.




### 클러스터링 알고리즘 비교

| 알고리즘 | 장점 | 단점 |
|----------|------|------|
| K-평균 클러스터링 | - 간단하고 이해하기 쉬움<br>- 대규모 데이터셋에서 효율적<br>- 다양한 분야에 적용 가능 | - 클러스터의 수를 미리 결정해야 함<br>- 이상치에 민감<br>- 클러스터가 구형이라는 가정 |
| 계층적 클러스터링 | - 클러스터 수를 미리 결정할 필요 없음<br>- 클러스터 계층 구조 제공 | - 대규모 데이터셋에 비효율적<br>- 이상치에 민감 |
| DBSCAN | - 클러스터의 수를 미리 결정할 필요 없음<br>- 이상치에 강함<br>- 임의의 형태의 클러스터 탐지 가능 | - 밀도 차이가 큰 데이터셋에서 성능 저하<br>- 파라미터 설정이 중요 |
| 스펙트럴 클러스터링 | - 비구형 클러스터에 효과적<br>- 다양한 유형의 데이터에 적용 가능 | - 대규모 데이터셋에 비효율적<br>- 파라미터 선택이 중요 |